# Guide complet : Chatbot IA sur h√©bergement mutualis√©

## Contexte du projet

Bruno souhaite installer un chatbot avec intelligence artificielle sur un h√©bergement mutualis√©. Ce document r√©capitule toutes les informations, d√©cisions et strat√©gies retenues pour ce projet.

---

## 1. Possibilit√© d'installation sur h√©bergement mutualis√©

### ‚úÖ C'est possible, mais avec limitations

**Solutions compatibles :**
- Chatbots bas√©s sur JavaScript (ex√©cution c√¥t√© client/navigateur)
- Chatbots via widgets tiers (Tidio, Drift, Intercom, Crisp)
- Chatbots PHP basiques

**Limitations de l'h√©bergement mutualis√© :**
- Pas d'acc√®s SSH ou de contr√¥le serveur complet
- Impossible d'installer des frameworks n√©cessitant Node.js, Python, ou autres environnements sp√©cifiques
- Ressources CPU/RAM tr√®s limit√©es (128-512 Mo g√©n√©ralement)
- Pas de websockets en temps r√©el dans la plupart des cas

### ‚ùå Impossibilit√© d'h√©berger une IA localement

**Pourquoi les IA l√©g√®res ne fonctionnent pas sur mutualis√© :**

**Ressources n√©cessaires** - M√™me les mod√®les les plus l√©gers requi√®rent :
- Plusieurs Go de RAM (minimum 2-4 Go)
- GPU ou CPU puissant pour l'inf√©rence
- Plusieurs Go d'espace disque pour les fichiers du mod√®le

**Environnement technique** - Les mod√®les IA n√©cessitent :
- Python avec biblioth√®ques sp√©cifiques (PyTorch, TensorFlow, transformers)
- Node.js ou autres environnements non-PHP
- Acc√®s syst√®me que le mutualis√© ne permet pas

**Performance** - M√™me si c'√©tait techniquement possible :
- Un mod√®le IA l√©ger prendrait 5-30 secondes pour r√©pondre
- Exp√©rience utilisateur inutilisable
- Risque de saturation des ressources serveur

---

## 2. Solution retenue : Architecture hybride

**Principe :**
- Frontend et interface web ‚Üí H√©bergement mutualis√©
- Intelligence artificielle ‚Üí APIs externes (h√©berg√©es ailleurs)
- Communication via appels HTTP/HTTPS

**Avantages :**
- Pas de limitation de ressources pour l'IA
- Mod√®les puissants et rapides
- Mise √† jour automatique des mod√®les
- Paiement √† l'usage ou gratuit selon services

**Sch√©ma de fonctionnement :**
```
Visiteur du site
    ‚Üì
Site web (h√©bergement mutualis√©)
    ‚Üì
Script PHP ou JavaScript
    ‚Üì
API IA externe (Groq, Gemini, etc.)
    ‚Üì
R√©ponse renvoy√©e au visiteur
```

---

## 3. Services IA disponibles et comparaison

### Services IA gratuits/peu co√ªteux √©tudi√©s

#### **Groq** ‚≠ê Service principal recommand√©

**Caract√©ristiques :**
- Ultra rapide (inf√©rence en quelques millisecondes)
- Utilise des mod√®les open source (Llama, Mixtral, Gemma)
- Excellent support multilingue dont fran√ßais

**Offre gratuite :**
- 30 requ√™tes par minute
- 14 400 requ√™tes par jour
- 14 400 tokens par minute
- 1 000 000 tokens par jour
- 10 requ√™tes simultan√©es maximum

**Mod√®les disponibles gratuitement :**
- Llama 3.1 (8B, 70B, 405B)
- Llama 3.3 (70B)
- Mixtral 8x7B
- Gemma 2 (9B, 27B)

**Limitations :**
- Pas de garantie de disponibilit√© (best effort)
- Peut √™tre plus lent aux heures de pointe
- Pas de support prioritaire
- Politique d'usage raisonnable

**Plan payant (si besoin futur) :**
- Pay-as-you-go (paiement √† l'usage)
- Environ 0,10-0,70 dollars par million de tokens selon mod√®le
- Limites beaucoup plus √©lev√©es

---

#### **Gemini** (Google AI) ‚≠ê Service backup recommand√©

**Caract√©ristiques :**
- Excellent support multilingue (fran√ßais inclus)
- Tr√®s bon avec contexte long
- Multimodal (texte + images)
- Int√©gration Google facilit√©e

**Offre gratuite :**
- 15 requ√™tes par minute
- 1 500 requ√™tes par jour
- 1 000 000 tokens par minute (√©norme !)

**Mod√®les disponibles gratuitement :**
- Gemini 1.5 Flash (rapide, l√©ger)
- Gemini 1.5 Pro (plus puissant)
- Gemini 2.0 Flash (le plus r√©cent)

**Points forts :**
- Tr√®s g√©n√©reux en tokens (1 million/minute)
- Excellente qualit√© en fran√ßais
- Capacit√© multimodale unique

**Points faibles :**
- Moins de requ√™tes/jour que Groq (1 500 vs 14 400)
- Moins de requ√™tes/minute que Groq (15 vs 30)

---

#### **Autres services disponibles**

**Cohere :**
- 1000 appels API par mois gratuitement
- Pas besoin de carte bancaire pour tester
- Support multilingue (fran√ßais)
- Prix comp√©titifs apr√®s gratuit
- Mod√®les : Generate, Command, Embed, Rerank

**Mistral AI** (fran√ßais üá´üá∑) :
- Entreprise fran√ßaise sp√©cialis√©e IA
- Excellents mod√®les pour le fran√ßais
- Offre gratuite limit√©e
- API professionnelle
- Prix raisonnables

**Hugging Face Inference API :**
- Offre gratuite g√©n√©reuse (avec rate limits)
- Acc√®s √† des milliers de mod√®les open source
- Mod√®les fran√ßais excellents (Mistral, Vigogne)
- Id√©al pour tester diff√©rents mod√®les
- Gratuit mais limit√© en requ√™tes/heure

**Together AI :**
- Sp√©cialis√© dans mod√®les open source
- Prix tr√®s comp√©titifs
- Bons mod√®les fran√ßais (Mistral)
- Cr√©dits gratuits au d√©marrage
- API facile √† utiliser

**Replicate :**
- Paiement √† l'usage uniquement
- Acc√®s √† nombreux mod√®les (Llama, Mistral)
- Pas d'abonnement mensuel
- Quelques centimes par requ√™te
- Bon pour usage occasionnel

**AI21 Labs :**
- Mod√®les Jurassic
- Offre gratuite d'essai
- Bonne qualit√© mais moins connu
- Prix moyens

---

### Tableau comparatif des services principaux

| Solution | Gratuit | Qualit√© fran√ßais | Vitesse | Facilit√© | Requ√™tes/jour |
|----------|---------|------------------|---------|----------|---------------|
| Groq | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | 14 400 |
| Gemini | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | 1 500 |
| Cohere | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ~33 |
| Mistral AI | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Variable |
| Hugging Face | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | Variable |
| Together AI | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê | Variable |

---

### Comparaison Groq vs Gemini (les 2 services retenus)

| Crit√®re | Groq | Gemini |
|---------|------|--------|
| Requ√™tes/minute | 30 | 15 |
| Requ√™tes/jour | 14 400 | 1 500 |
| Tokens/minute | 14 400 | 1 000 000 |
| Vitesse | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Qualit√© fran√ßais | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê |
| Multimodal | ‚ùå | ‚úÖ (texte + images) |

**Conclusion :**
- **Groq** : Meilleur si beaucoup de visiteurs (plus de requ√™tes/jour)
- **Gemini** : Meilleur si conversations longues (plus de tokens) et besoin d'images

---

## 4. Diff√©rence entre requ√™tes et tokens

### D√©finition des requ√™tes

**Requ√™te (Request) = 1 appel √† l'API**, peu importe la longueur du texte

**Exemples :**
- Visiteur dit "Bonjour" ‚Üí 1 requ√™te
- Visiteur dit "Peux-tu m'expliquer en d√©tail toute l'histoire de France depuis la pr√©histoire jusqu'√† aujourd'hui avec tous les d√©tails possibles ?" ‚Üí 1 requ√™te aussi

**Donc : 1 requ√™te = 1 √©change**, quelle que soit la taille du message.

---

### D√©finition des tokens

**Token = un morceau de mot** (environ 4 caract√®res en fran√ßais, 3-4 lettres)

**Exemples de d√©coupage :**
- "Bonjour" = environ 2 tokens
- "Chatbot" = environ 2 tokens
- "Intelligence artificielle" = environ 5 tokens
- Une phrase de 100 mots = environ 130-150 tokens en fran√ßais

**R√®gle approximative :**
- 1 token ‚âà 4 caract√®res en fran√ßais
- 100 mots ‚âà 130-150 tokens
- 1000 caract√®res ‚âà 250 tokens

---

### Calcul des tokens par requ√™te

**√Ä chaque appel API, les tokens comptabilis√©s sont :**

```
Tokens totaux = Tokens INPUT + Tokens OUTPUT

INPUT = Question/message envoy√© √† l'IA (y compris historique)
OUTPUT = R√©ponse g√©n√©r√©e par l'IA
```

**Exemple concret :**

Message utilisateur : "Quel temps fait-il ?" (5 tokens)
R√©ponse IA : "Il fait beau et ensoleill√© aujourd'hui avec 22 degr√©s." (15 tokens)

**R√©sultat :**
- 1 requ√™te consomm√©e
- 20 tokens consomm√©s (5 input + 15 output)

---

### Pourquoi cette double limite ?

**Limite de requ√™tes :** 
- √âvite le spam et les abus
- Emp√™che quelqu'un de faire 1000 appels en 1 minute
- Prot√®ge le service contre la surcharge

**Limite de tokens :**
- √âvite l'abus de ressources de calcul
- Emp√™che des requ√™tes g√©antes qui satureraient le syst√®me
- Contr√¥le la consommation r√©elle de puissance de calcul

---

### Application pratique pour le chatbot

**Avec Groq gratuit :**
- 30 requ√™tes/min = 30 visiteurs peuvent poser 1 question par minute
- 14 400 tokens/min = si chaque √©change fait 500 tokens, environ 28 √©changes par minute possible

**Avec Gemini gratuit :**
- 15 requ√™tes/min = 15 visiteurs peuvent poser 1 question par minute
- 1 000 000 tokens/min = √©norme ! On atteint la limite de requ√™tes bien avant celle des tokens

**Conclusion importante :**
Pour un chatbot classique, on atteint g√©n√©ralement **la limite de requ√™tes AVANT celle des tokens**, sauf si on fait des conversations tr√®s longues avec beaucoup d'historique.

---

## 5. Strat√©gie multi-API : cumul des quotas

### Principe du cumul

**Chaque service a ses propres compteurs ind√©pendants**

Les quotas ne se partagent PAS entre services ‚Üí on peut r√©ellement les cumuler !

**Exemple :**
- Groq : 30 req/min + 14 400 tokens/min
- Gemini : 15 req/min + 1 000 000 tokens/min
- Cohere : 1000 req/mois

**En utilisant les 3 simultan√©ment, on ne touche pas aux quotas des autres.**

---

### Strat√©gies de cumul possibles

#### 1. Rotation simple (Round-robin)
Alternance entre les services dans un ordre fixe.

```
Requ√™te 1 ‚Üí Groq
Requ√™te 2 ‚Üí Gemini
Requ√™te 3 ‚Üí Cohere
Requ√™te 4 ‚Üí Groq (on recommence)
```

**Avantages :** R√©partition √©quitable
**Inconv√©nients :** Pas d'optimisation par type de requ√™te

---

#### 2. Fallback (Solution de secours) ‚≠ê **RECOMMAND√â**

Utilise toujours le service principal, bascule sur backup si limite atteinte.

```
Toujours essayer Groq (le plus rapide)
    ‚Üì (si limite atteinte erreur 429)
Basculer automatiquement sur Gemini
    ‚Üì (si limite atteinte)
Basculer sur Cohere (dernier recours)
```

**Avantages :**
- Utilise le service pr√©f√©r√©/plus rapide en priorit√©
- Haute disponibilit√© garantie
- Simple √† impl√©menter

**Inconv√©nients :**
- Service principal peut √™tre plus sollicit√©

---

#### 3. Selon le type de requ√™te

Choix intelligent du service selon la nature de la question.

```
Questions courtes/simples ‚Üí Groq (ultra rapide)
Questions longues/contexte important ‚Üí Gemini (plus de tokens)
Questions complexes n√©cessitant raisonnement ‚Üí Mistral
Questions avec images ‚Üí Gemini (seul multimodal)
```

**Avantages :** Optimisation maximale selon besoin
**Inconv√©nients :** Logique complexe √† impl√©menter

---

#### 4. Load balancing intelligent

V√©rifie les quotas restants en temps r√©el et choisit le meilleur service disponible.

**Principe :**
- V√©rifie combien de requ√™tes restent sur chaque service
- Choisit celui qui a le plus de marge
- R√©partit intelligemment la charge

**Avantages :** Utilisation optimale des quotas
**Inconv√©nients :** Complexe, n√©cessite tracking pr√©cis

---

### Capacit√© totale cumul√©e (services gratuits)

**Avec Groq + Gemini + Cohere :**

**Par jour :**
- Groq : 14 400 requ√™tes
- Gemini : 1 500 requ√™tes
- Cohere : 33 requ√™tes (environ 1000/mois √∑ 30 jours)
- **TOTAL : environ 15 933 requ√™tes par jour**

**Par minute :**
- Groq : 30 requ√™tes/min
- Gemini : 15 requ√™tes/min
- Cohere : limit√© au mois
- **TOTAL : 45 requ√™tes/min** si on alterne

**Par minute (tokens) :**
- Groq : 14 400 tokens/min
- Gemini : 1 000 000 tokens/min
- **TOTAL : 1 014 400 tokens/min**

---

### Avantages de la strat√©gie multi-API

‚úÖ Multiplie consid√©rablement les capacit√©s gratuites
‚úÖ Haute disponibilit√© (si un service est down, les autres prennent le relais)
‚úÖ Optimisation des co√ªts (reste gratuit plus longtemps)
‚úÖ Flexibilit√© (possibilit√© de tester diff√©rents mod√®les)
‚úÖ R√©silience (pas de point unique de d√©faillance)

### Inconv√©nients

‚ùå Code plus complexe √† maintenir
‚ùå Coh√©rence des r√©ponses peut varier entre mod√®les IA
‚ùå Gestion de plusieurs cl√©s API
‚ùå N√©cessite tracking des quotas
‚ùå Debugging plus difficile

---

### Recommandation finale pour Bruno

**Configuration retenue : Groq (principal) + Gemini (backup)**

**Strat√©gie : Fallback automatique**
- Groq en priorit√© (ultra rapide, g√©n√©reux)
- Gemini en backup (excellent fran√ßais, √©norme capacit√© tokens)
- Possibilit√© d'ajouter Cohere/Mistral plus tard si besoin

**R√©sultat :**
- Environ 16 000 requ√™tes par jour gratuitement
- 45 requ√™tes par minute en alternant
- Haute disponibilit√© garantie
- Simplicit√© d'impl√©mentation

---

## 6. Gestion de l'historique de conversation

### Probl√©matique fondamentale

**Les APIs d'IA n'ont AUCUNE m√©moire entre les requ√™tes.**

Chaque appel API est totalement ind√©pendant. Si tu ne fournis pas le contexte, l'IA ne saura pas de quoi on a parl√© avant.

**Cons√©quence :**
Pour maintenir une conversation coh√©rente, il faut envoyer **TOUT l'historique** √† chaque nouvelle requ√™te.

---

### Architecture de gestion de l'historique

**Principe g√©n√©ral :**

```
1. Stockage persistant (BDD ou session PHP)
   ‚Üì
2. √Ä chaque nouvelle question du visiteur :
   - R√©cup√®re TOUT l'historique pr√©c√©dent
   - Ajoute la nouvelle question
   - Envoie TOUT √† l'API IA
   ‚Üì
3. Re√ßoit la r√©ponse de l'IA
   ‚Üì
4. Stocke la r√©ponse dans l'historique
   ‚Üì
5. Affiche au visiteur
```

---

### Format standard de l'historique

**Format JSON utilis√© par toutes les APIs IA :**

Structure de base d'un message :
```
{
  "role": "user" ou "assistant" ou "system",
  "content": "Le texte du message"
}
```

**Exemple d'historique complet :**

```
Conversation :
[
  {
    "role": "system",
    "content": "Tu es un assistant commercial sp√©cialis√© en h√¥tellerie"
  },
  {
    "role": "user",
    "content": "Bonjour, je cherche un h√¥tel √† Paris"
  },
  {
    "role": "assistant",
    "content": "Bonjour ! Je peux vous aider. Quel est votre budget ?"
  },
  {
    "role": "user",
    "content": "Environ 150‚Ç¨ par nuit"
  },
  {
    "role": "assistant",
    "content": "Parfait, voici quelques suggestions dans cette gamme de prix..."
  }
]
```

**R√¥les expliqu√©s :**
- **system** : Instructions/contexte pour l'IA (optionnel, envoy√© une seule fois au d√©but)
- **user** : Messages du visiteur/utilisateur
- **assistant** : R√©ponses de l'IA

**Ce format est compatible avec :**
- OpenAI (GPT)
- Anthropic (Claude)
- Groq
- Gemini
- Cohere
- Mistral
- Pratiquement toutes les APIs IA modernes

---

### Options de stockage de l'historique

#### Option 1 : Session PHP (simple, petits volumes)

**Avantages :**
- Simple √† mettre en place
- Pas besoin de base de donn√©es
- Rapide
- Automatiquement nettoy√© apr√®s expiration session

**Inconv√©nients :**
- Perdu si session expire ou navigateur ferm√©
- Pas de persistance long terme
- Limit√© en taille (quelques Mo maximum)
- Impossible d'analyser les conversations pass√©es
- Pas adapt√© si plusieurs serveurs (load balancing)

**Quand l'utiliser :**
- Chatbot simple avec peu de trafic
- Pas besoin de conserver l'historique
- Phase de test/d√©veloppement

---

#### Option 2 : Base de donn√©es MySQL ‚≠ê **RECOMMAND√â**

**Structure de table sugg√©r√©e :**

```
Table : conversations

Colonnes :
- id : INT AUTO_INCREMENT PRIMARY KEY
- session_id : VARCHAR(255) - Identifiant unique de la conversation
- role : ENUM('user', 'assistant', 'system') - Qui parle
- content : TEXT - Contenu du message
- ai_service : VARCHAR(50) - Quel service IA a r√©pondu (groq, gemini, etc.)
- tokens_used : INT - Nombre de tokens consomm√©s (pour statistiques)
- created_at : TIMESTAMP DEFAULT CURRENT_TIMESTAMP - Date/heure du message

Index :
- INDEX sur session_id (pour r√©cup√©ration rapide)
- INDEX sur created_at (pour analyses temporelles)
```

**Avantages :**
- Persistance totale des conversations
- Possibilit√© d'analyses et statistiques
- Support de volumes importants
- Possibilit√© de reprendre conversation ult√©rieurement
- Tracking pr√©cis de l'utilisation
- Backup et r√©cup√©ration possibles

**Inconv√©nients :**
- L√©g√®rement plus complexe √† mettre en place
- N√©cessite une base de donn√©es
- Consomme de l'espace disque

**Quand l'utiliser :**
- Production
- Besoin de conserver les conversations
- Analyse des usages
- Support client
- Am√©lioration continue du chatbot

---

### Gestion du basculement entre APIs

**Point crucial :** Peu importe quelle IA r√©pond, elle doit avoir acc√®s au m√™me historique complet.

**Comment √ßa fonctionne avec le basculement :**

```
Conversation en cours :
Message 1-5 ‚Üí R√©pondu par Groq
Message 6 ‚Üí Groq atteint sa limite (erreur 429)
Message 6 ‚Üí Automatiquement bascul√© sur Gemini

IMPORTANT : Gemini re√ßoit TOUT l'historique (messages 1-6)
            m√™me si messages 1-5 ont √©t√© trait√©s par Groq

R√©sultat : Gemini comprend le contexte et r√©pond de mani√®re coh√©rente
```

**Principe cl√© :**
L'historique est **agnostique du service IA**. On stocke juste les √©changes user/assistant, peu importe quel service IA a g√©n√©r√© la r√©ponse.

**Avantage :**
Transition transparente entre services, l'utilisateur ne remarque rien.

---

## 7. PROBL√àME MAJEUR : Consommation des tokens par l'historique

### Le probl√®me expliqu√©

**L'historique consomme des tokens √† chaque requ√™te !**

**Rappel du calcul des tokens :**
```
Tokens totaux par requ√™te = INPUT + OUTPUT

INPUT = Historique complet + Nouvelle question
OUTPUT = R√©ponse de l'IA
```

**Exemple d'explosion des tokens :**

**Message 1 :**
- User : "Bonjour" (2 tokens)
- IA : "Bonjour ! Comment puis-je vous aider ?" (10 tokens)
- **Co√ªt total : 2 + 10 = 12 tokens**

**Message 2 :**
- Historique √† envoyer : 12 tokens
- User : "Quel temps fait-il ?" (5 tokens)
- IA : "Je ne peux pas v√©rifier la m√©t√©o en temps r√©el" (15 tokens)
- **Co√ªt total : 12 + 5 + 15 = 32 tokens**

**Message 3 :**
- Historique √† envoyer : 32 tokens
- User : "D'accord, merci" (4 tokens)
- IA : "De rien, bonne journ√©e !" (6 tokens)
- **Co√ªt total : 32 + 4 + 6 = 42 tokens**

**Message 10 :**
- Historique cumul√© : peut atteindre 500-1000 tokens !
- **√áa augmente exponentiellement ! üöÄ**

---

### Impact r√©el sur les quotas

**Sans gestion/optimisation de l'historique :**

**Groq (14 400 tokens/min) :**
- Conversation moyenne de 10 messages = 500-1000 tokens
- Capacit√© r√©elle : environ 15-30 conversations par minute seulement
- Au lieu des 30 requ√™tes th√©oriques

**Avec optimisation :**
- M√™me quota de 14 400 tokens/min
- Historique ma√Ætris√© √† 100-200 tokens par requ√™te
- Capacit√© r√©elle : 100-200 conversations par minute
- **Multiplication par 5-10 de la capacit√© !**

**Conclusion importante :**
Sans optimisation de l'historique, on divise la capacit√© r√©elle par 5 √† 10 fois !

---

### Strat√©gies d'optimisation de l'historique

#### Strat√©gie 1 : Limite simple du nombre de messages

**Principe :**
Garder seulement les N derniers messages (ex: 10-12 derniers messages).

**Fonctionnement :**
- On garde les 12 derniers messages de l'historique
- On supprime les plus anciens
- Cela repr√©sente environ 6 √©changes (user + assistant)

**Avantages :**
- Tr√®s simple √† impl√©menter
- Efficace imm√©diatement
- √âconomie de 60-70% des tokens

**Inconv√©nients :**
- Perd le contexte ancien de la conversation
- Peut perdre des informations importantes mentionn√©es au d√©but

**Quand l'utiliser :**
- Conversations courtes/moyennes
- Chatbot FAQ simple
- Phase de d√©marrage

**Param√®tres recommand√©s :**
- MAX_HISTORY_MESSAGES = 12 (6 √©changes)
- Ajustable selon le type de conversation

---

#### Strat√©gie 2 : Syst√®me de fen√™tre glissante avec r√©sum√©

**Principe :**
Quand l'historique devient trop long, on r√©sume les messages anciens.

**Fonctionnement :**
1. Quand on atteint 20+ messages
2. On prend les 10 premiers messages
3. On appelle l'IA pour g√©n√©rer un r√©sum√© court de ces messages
4. On remplace les 10 messages par le r√©sum√©
5. On garde les 10 derniers messages tels quels

**Avantages :**
- Conserve le contexte important de toute la conversation
- √âconomie de 40-50% des tokens
- Meilleure qualit√© de conversation longue

**Inconv√©nients :**
- Tr√®s complexe √† impl√©menter
- Co√ªte des tokens suppl√©mentaires pour g√©n√©rer les r√©sum√©s
- Risque de perte d'informations dans le r√©sum√©
- N√©cessite logique sophistiqu√©e

**Quand l'utiliser :**
- Conversations tr√®s longues (support client)
- Chatbot complexe n√©cessitant beaucoup de contexte
- Phase avanc√©e du projet

---

#### Strat√©gie 3 : Compression intelligente ‚≠ê **RECOMMAND√â**

**Principe :**
Combinaison de plusieurs techniques pour optimiser sans perdre le contexte essentiel.

**R√®gles appliqu√©es :**
1. **Toujours garder le message syst√®me** (contexte/instructions initiales de l'IA)
2. **Garder les 2-4 premiers √©changes** (contexte d'introduction important)
3. **Garder les 8-10 derniers messages** (contexte r√©cent, le plus important)
4. **Limite maximale de tokens** (ex: 1500-2000 tokens)
5. **Ajustement dynamique** si on d√©passe la limite

**Avantages :**
- √âconomie de 50-60% des tokens
- Garde le contexte essentiel (d√©but + r√©cent)
- Complexit√© raisonnable
- √âquilibre optimal qualit√©/performance

**Inconv√©nients :**
- Un peu plus complexe que la limite simple
- N√©cessite estimation des tokens

**Quand l'utiliser :**
- Production, usage r√©el
- Conversations moyennes √† longues
- Meilleur compromis pour la plupart des cas

---

### Tableau comparatif des strat√©gies

| Approche | Tokens √©conomis√©s | Contexte pr√©serv√© | Complexit√© | Recommandation |
|----------|-------------------|-------------------|------------|----------------|
| Aucune optimisation | 0% | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê | ‚ùå Ne jamais faire |
| Limite simple (12 msg) | 60-70% | ‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê | ‚úÖ Bon pour d√©marrer |
| Compression intelligente | 50-60% | ‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê | ‚≠ê RECOMMAND√â |
| R√©sum√© automatique | 40-50% | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê | ‚ö†Ô∏è Avanc√© uniquement |

---

### Param√®tres recommand√©s pour la compression intelligente

**Constantes √† d√©finir :**

```
MAX_HISTORY_MESSAGES = 12
  ‚Üí Nombre maximum de messages √† conserver
  ‚Üí Repr√©sente environ 6 √©changes (user + assistant)
  
MAX_TOKENS = 2000
  ‚Üí Limite maximale de tokens pour l'historique
  ‚Üí S√©curit√© suppl√©mentaire si messages tr√®s longs
  
KEEP_FIRST_EXCHANGES = 4
  ‚Üí Garde les 2 premiers √©changes (4 messages)
  ‚Üí Conserve le contexte d'introduction
  
KEEP_RECENT_MESSAGES = 8
  ‚Üí Garde les 8 derniers messages
  ‚Üí Conserve le contexte le plus r√©cent et pertinent
```

**Logique d'optimisation :**

1. Si l'historique complet fait moins de 12 messages ‚Üí On garde tout
2. Si l'historique d√©passe 12 messages :
   - On garde le message syst√®me (s'il existe)
   - On garde les 2-4 premiers √©changes
   - On garde les 8 derniers messages
   - On supprime le milieu
3. On estime les tokens du r√©sultat
4. Si √ßa d√©passe encore 2000 tokens :
   - On r√©duit davantage (garde seulement les 8 derniers)

---

### Estimation des tokens

**M√©thode d'estimation rapide :**

**R√®gle simple :**
- 1 token ‚âà 4 caract√®res en fran√ßais
- 1000 caract√®res ‚âà 250 tokens

**M√©thode de calcul :**
1. Convertir l'historique en JSON (format texte)
2. Compter le nombre de caract√®res
3. Diviser par 4

**Exemple :**
```
Historique JSON = 2000 caract√®res
Estimation tokens = 2000 / 4 = 500 tokens
```

**Pr√©cision :**
- Cette estimation est approximative mais suffisante
- Erreur de ¬±10-20% acceptable pour l'optimisation
- Les APIs fournissent le compte exact apr√®s traitement

---

### Monitoring et tracking des tokens

**Informations √† logger/tracker :**

**Par requ√™te :**
- Tokens INPUT (historique + question)
- Tokens OUTPUT (r√©ponse)
- Tokens TOTAL
- Service IA utilis√©
- Timestamp
- Session ID

**Statistiques globales :**
- Consommation par jour/heure
- Consommation par service IA
- Conversations les plus gourmandes
- Taille moyenne des historiques

**Alertes √† mettre en place :**
- Si une conversation d√©passe 500 tokens ‚Üí Warning
- Si on approche des limites quotidiennes ‚Üí Alerte
- Si taux d'erreur 429 (rate limit) augmente ‚Üí Alerte

**B√©n√©fices du monitoring :**
- Anticiper les d√©passements de quotas
- Optimiser davantage si n√©cessaire
- Comprendre les patterns d'utilisation
- D√©tecter les abus potentiels

---

## 8. Architecture compl√®te du syst√®me

### Vue d'ensemble des composants

**Frontend (interface utilisateur) :**
- Page HTML avec zone de chat
- JavaScript pour interactions temps r√©el
- CSS pour design
- H√©berg√© sur serveur mutualis√©

**Backend (logique serveur) :**
- Scripts PHP sur h√©bergement mutualis√©
- Gestion des requ√™tes utilisateur
- Coordination avec APIs IA
- Gestion de l'historique

**Base de donn√©es :**
- MySQL pour stockage historique
- Table conversations
- Table optionnelle pour statistiques

**APIs externes :**
- Groq (service principal)
- Gemini (service backup)
- Autres si ajout√©s plus tard

---

### Flux de fonctionnement complet

**√âtape 1 : Visiteur envoie un message**
```
1. Visiteur tape un message dans le chat
2. JavaScript intercepte l'envoi
3. Requ√™te AJAX vers script PHP
```

**√âtape 2 : Backend r√©cup√®re l'historique**
```
4. PHP re√ßoit le message + session_id
5. Requ√™te SQL : r√©cup√®re tout l'historique de cette session
6. L'historique est charg√© en m√©moire
```

**√âtape 3 : Optimisation de l'historique**
```
7. Applique compression intelligente
   - Garde message syst√®me
   - Garde premiers √©changes
   - Garde messages r√©cents
   - Supprime le milieu si trop long
8. Estime les tokens
9. Ajuste si d√©passe MAX_TOKENS
```

**√âtape 4 : Tentative d'appel API principal (Groq)**
```
10. Pr√©pare requ√™te HTTP vers API Groq
11. Envoie historique optimis√© + nouveau message
12. Attend r√©ponse
```

**√âtape 5a : Si Groq r√©pond (cas normal)**
```
13. Re√ßoit r√©ponse de Groq
14. Parse la r√©ponse JSON
15. Passe √† l'√©tape 6
```

**√âtape 5b : Si Groq est en limite (erreur 429)**
```
13. Re√ßoit erreur 429 (rate limit exceeded)
14. D√©tecte l'erreur
15. Bascule automatiquement sur Gemini
16. Envoie la m√™me requ√™te √† Gemini
17. Re√ßoit r√©ponse de Gemini
18. Passe √† l'√©tape 6
```

**√âtape 6 : Sauvegarde en base de donn√©es**
```
19. Ins√®re le message utilisateur en BDD
    - session_id
    - role = 'user'
    - content = message
20. Ins√®re la r√©ponse IA en BDD
    - session_id
    - role = 'assistant'
    - content = r√©ponse
    - ai_service = 'groq' ou 'gemini'
    - tokens_used = nombre estim√©
```

**√âtape 7 : Retour au visiteur**
```
21. PHP renvoie la r√©ponse en JSON
22. JavaScript re√ßoit la r√©ponse
23. Affiche la r√©ponse dans le chat
24. Interface pr√™te pour prochain message
```

---

### Gestion des sessions

**Identification des conversations :**

**Session ID :**
- Identifiant unique par visiteur/conversation
- G√©n√©r√© au premier message
- Stock√© en cookie ou localStorage
- Permet de retrouver l'historique

**G√©n√©ration du session_id :**
- Format : combinaison date + hash al√©atoire
- Exemple : "chat_20250122_a7b3c9d2e1f4"
- Unique et tra√ßable

**Dur√©e de vie :**
- Session active : tant que visiteur sur le site
- Session persistante : peut durer plusieurs jours/semaines
- Nettoyage : suppression des sessions > 30 jours (optionnel)

---

### S√©curit√© et bonnes pratiques

**Protection des cl√©s API :**
- JAMAIS dans le code JavaScript (visible par tous)
- Stock√©es dans fichier PHP s√©par√© (hors dossier web)
- Variables d'environnement ou fichier config.php
- Exclusion du fichier de config du Git (.gitignore)

**Validation des entr√©es utilisateur :**
- V√©rifier longueur maximum des messages
- Bloquer caract√®res sp√©ciaux dangereux
- Limiter fr√©quence d'envoi (anti-spam)
- Sanitization avant sauvegarde BDD

**Rate limiting c√¥t√© serveur :**
- Limiter nombre de messages par IP/session
- Exemple : max 30 messages par minute par IP
- Protection contre abus et spam
- √âconomie des quotas API

**Sanitization des r√©ponses :**
- √âchapper HTML dans les r√©ponses IA
- Pr√©venir injection XSS
- Validation avant affichage

**HTTPS obligatoire :**
- Chiffrement des communications
- Protection des donn√©es √©chang√©es
- Standard pour APIs modernes

---

## 9. Checklist de mise en ≈ìuvre

### Configuration initiale

**Comptes et cl√©s API :**
- [ ] Cr√©er compte Groq sur groq.com
- [ ] G√©n√©rer cl√© API Groq
- [ ] Cr√©er compte Google Cloud pour Gemini
- [ ] Activer Gemini API
- [ ] G√©n√©rer cl√© API Gemini
- [ ] (Optionnel) Cr√©er compte Cohere
- [ ] (Optionnel) G√©n√©rer cl√© API Cohere

**Base de donn√©es :**
- [ ] Cr√©er base de donn√©es MySQL sur h√©bergement
- [ ] Noter les identifiants de connexion
- [ ] Cr√©er table conversations (structure fournie)
- [ ] Tester connexion PHP ‚Üí MySQL
- [ ] Cr√©er index sur session_id

**H√©bergement :**
- [ ] V√©rifier version PHP (minimum 7.4, id√©alement 8.0+)
- [ ] V√©rifier extension curl activ√©e
- [ ] V√©rifier extension json activ√©e
- [ ] V√©rifier extension mysqli ou PDO activ√©e
- [ ] Tester appels HTTPS externes

---

### D√©veloppement PHP

**Fichiers √† cr√©er :**
- [ ] config.php (cl√©s API, config BDD)
- [ ] chatbot.php (classe principale)
- [ ] api_groq.php (gestion API Groq)
- [ ] api_gemini.php (gestion API Gemini)
- [ ] database.php (gestion BDD)
- [ ] history_manager.php (gestion historique)
- [ ] endpoint.php (point d'entr√©e AJAX)

**Fonctionnalit√©s √† impl√©menter :**
- [ ] Connexion base de donn√©es
- [ ] Fonction r√©cup√©ration historique
- [ ] Fonction sauvegarde message
- [ ] Compression intelligente historique
- [ ] Estimation tokens
- [ ] Appel API Groq
- [ ] Appel API Gemini
- [ ] Syst√®me fallback automatique
- [ ] Gestion erreurs 429 (rate limit)
- [ ] Logging des requ√™tes
- [ ] Tracking tokens consomm√©s

---

### D√©veloppement Frontend

**Fichiers √† cr√©er :**
- [ ] index.html (page principale)
- [ ] chat.css (styles du chat)
- [ ] chat.js (logique JavaScript)

**Fonctionnalit√©s √† impl√©menter :**
- [ ] Interface chat (messages, input)
- [ ] G√©n√©ration/r√©cup√©ration session_id
- [ ] Envoi message via AJAX
- [ ] Affichage r√©ponse
- [ ] Indicateur "IA en train d'√©crire..."
- [ ] Gestion erreurs r√©seau
- [ ] Scroll automatique vers bas
- [ ] (Optionnel) Mise en forme Markdown des r√©ponses

---

### Optimisations

**Compression historique :**
- [ ] D√©finir MAX_HISTORY_MESSAGES = 12
- [ ] D√©finir MAX_TOKENS = 2000
- [ ] Impl√©menter fonction estimateTokens()
- [ ] Impl√©menter fonction prepareForAI()
- [ ] Impl√©menter fonction smartTrim() (optionnel)

**Performance :**
- [ ] Activer cache PHP opcache
- [ ] Minimiser fichiers CSS/JS
- [ ] Utiliser CDN pour biblioth√®ques externes
- [ ] Index BDD optimis√©s

---

### S√©curit√©

**Protection API :**
- [ ] Cl√©s API dans fichier s√©par√© hors web root
- [ ] Fichier config.php exclu de Git (.gitignore)
- [ ] V√©rification origin/referer dans endpoint.php

**Protection utilisateur :**
- [ ] Validation longueur messages (max 500-1000 caract√®res)
- [ ] Rate limiting : max 30 messages/minute par IP
- [ ] Sanitization SQL (requ√™tes pr√©par√©es)
- [ ] √âchappement HTML dans affichage

**HTTPS :**
- [ ] Certificat SSL activ√© sur h√©bergement
- [ ] Force HTTPS dans .htaccess
- [ ] V√©rification HTTPS dans appels API

---

### Tests

**Tests fonctionnels :**
- [ ] Test conversation simple (3-4 √©changes)
- [ ] Test conversation longue (15+ √©changes)
- [ ] Test basculement Groq ‚Üí Gemini (simuler limite)
- [ ] Test reprise conversation apr√®s rechargement page
- [ ] Test historique correctement sauvegard√©
- [ ] Test compression historique fonctionne
- [ ] Test affichage messages corrects

**Tests limites :**
- [ ] Test rate limiting fonctionne
- [ ] Test message trop long refus√©
- [ ] Test erreur r√©seau g√©r√©e
- [ ] Test API indisponible g√©r√©e
- [ ] Test toutes APIs en limite

**Tests s√©curit√© :**
- [ ] Test injection SQL bloqu√©e
- [ ] Test XSS bloqu√©e
- [ ] Test spam bloqu√©
- [ ] Test acc√®s direct endpoint.php

---

### Monitoring et production

**Logs :**
- [ ] Logger toutes les requ√™tes API
- [ ] Logger erreurs 429
- [ ] Logger consommation tokens
- [ ] Fichier logs rotatif (ne pas remplir disque)

**Statistiques :**
- [ ] Dashboard consommation tokens/jour
- [ ] Graphique r√©partition Groq/Gemini
- [ ] Nombre conversations/jour
- [ ] Dur√©e moyenne conversations

**Alertes :**
- [ ] Alerte si proche limite quotas
- [ ] Alerte si taux erreur √©lev√©
- [ ] Alerte si temps r√©ponse lent

**Maintenance :**
- [ ] Script nettoyage vieilles conversations (>30j)
- [ ] Backup base de donn√©es r√©gulier
- [ ] Mise √† jour cl√©s API si n√©cessaire

---

## 10. Estimations de capacit√© finale

### Avec configuration optimis√©e (Groq + Gemini)

**Capacit√© th√©orique (quotas cumul√©s) :**
- 15 900 requ√™tes par jour
- 45 requ√™tes par minute

**Capacit√© r√©elle (avec optimisation historique) :**

**Conversations courtes (3-4 √©changes, ~200 tokens) :**
- Environ 16 000 conversations par jour
- Largement dans les limites

**Conversations moyennes (10-15 √©changes, ~500 tokens) :**
- Environ 4 000-6 000 conversations par jour
- Tr√®s confortable

**Conversations longues (30+ √©changes, ~1000 tokens) :**
- Environ 1 000-2 000 conversations par jour
- Encore tr√®s correct

---

### √âvolution et scalabilit√©

**Phase 1 : D√©marrage (gratuit)**
- Groq + Gemini uniquement
- 100% gratuit
- Capacit√© : plusieurs milliers de conversations/jour
- Suffisant pour 95% des sites

**Phase 2 : Croissance (si n√©cessaire)**
- Ajout Cohere en 3√®me backup
- Ajout Mistral AI si besoin fran√ßais premium
- Toujours gratuit avec quotas cumul√©s
- Capacit√© multipli√©e

**Phase 3 : Production intensive (si forte croissance)**
- Passage plan payant sur service principal
- Co√ªt estim√© : 5-20‚Ç¨/mois pour site moyen
- Capacit√© quasi-illimit√©e

---

## 11. Points d'attention importants

### Limites techniques √† conna√Ætre

**L'IA ne peut pas :**
- Acc√©der √† des bases de donn√©es externes en temps r√©el
- Naviguer sur internet (sauf si API sp√©cifique fournie)
- Ex√©cuter du code ou faire des calculs complexes pr√©cis
- Conserver des informations entre sessions sans historique
- Garantir 100% d'exactitude factuelle

**L'IA peut :**
- Converser naturellement
- R√©pondre √† des questions g√©n√©rales
- Aider √† r√©soudre des probl√®mes
- Fournir des explications
- S'adapter au contexte de la conversation

---

### Gestion des attentes utilisateurs

**Temps de r√©ponse :**
- Groq : tr√®s rapide (1-3 secondes)
- Gemini : rapide (2-5 secondes)
- Afficher indicateur de chargement important

**Qualit√© des r√©ponses :**
- Peut varier entre mod√®les
- Tester et ajuster si n√©cessaire
- Pr√©voir message syst√®me personnalis√©

**Limitations √† communiquer :**
- Informer utilisateurs que c'est une IA
- Pas d'acc√®s temps r√©el √† donn√©es externes
- Peut faire des erreurs
- Pr√©voir disclaimer appropri√©

---

### Conformit√© et l√©galit√©

**RGPD (si visiteurs europ√©ens) :**
- Informer de la collecte des conversations
- Permettre suppression des donn√©es
- Politique de confidentialit√©
- Dur√©e conservation limit√©e

**Mod√©ration du contenu :**
- Les APIs ont filtres int√©gr√©s
- Pr√©voir gestion contenus inappropri√©s
- Possibilit√© blacklist mots-cl√©s

**Responsabilit√© :**
- L'IA peut g√©n√©rer contenu incorrect
- Ajouter disclaimer appropri√©
- Ne pas utiliser pour conseil m√©dical/l√©gal/financier sans pr√©cautions

---

## 12. Ressources et documentation

### Documentation officielle des APIs

**Groq :**
- Documentation : https://console.groq.com/docs
- Playground : https://console.groq.com/playground
- Mod√®les disponibles : https://console.groq.com/docs/models

**Gemini (Google) :**
- Documentation : https://ai.google.dev/docs
- API Reference : https://ai.google.dev/api
- Quickstart : https://ai.google.dev/tutorials/quickstart

**Cohere :**
- Documentation : https://docs.cohere.com/
- API Reference : https://docs.cohere.com/reference/
- Playground : https://dashboard.cohere.com/playground

**Mistral AI :**
- Documentation : https://docs.mistral.ai/
- API : https://docs.mistral.ai/api/
- Mod√®les : https://docs.mistral.ai/getting-started/models/

---

### Communaut√©s et support

**Forums et discussions :**
- Stack Overflow (tag : groq, gemini-api, chatbot)
- Reddit : r/ArtificialIntelligence, r/ChatGPT
- Discord communautaires des diff√©rents services

**Tutoriels vid√©o :**
- YouTube : rechercher "Groq API tutorial"
- YouTube : rechercher "Gemini API tutorial"
- Cours Udemy/Coursera sur chatbots

---

## 13. Prochaines √©tapes recommand√©es

### Ordre de mise en ≈ìuvre sugg√©r√©

**Semaine 1 : Pr√©paration**
1. Cr√©er comptes Groq et Gemini
2. G√©n√©rer cl√©s API
3. √âtudier documentation de base
4. Pr√©parer base de donn√©es

**Semaine 2 : D√©veloppement backend**
5. Cr√©er structure fichiers PHP
6. Impl√©menter connexion BDD
7. Impl√©menter appel API Groq
8. Tester appel basique

**Semaine 3 : Historique et optimisation**
9. Impl√©menter sauvegarde historique
10. Impl√©menter r√©cup√©ration historique
11. Impl√©menter compression intelligente
12. Tester conversations longues

**Semaine 4 : Frontend et finitions**
13. Cr√©er interface chat
14. Connecter frontend ‚Üî backend
15. Impl√©menter fallback Gemini
16. Tests complets

**Semaine 5 : Production**
17. S√©curisation finale
18. Monitoring et logs
19. Mise en ligne
20. Tests utilisateurs r√©els

---

## Conclusion

Ce guide complet fournit toutes les informations th√©oriques et strat√©giques n√©cessaires pour mettre en place un chatbot IA sur h√©bergement mutualis√©.

**Points cl√©s √† retenir :**

‚úÖ **Architecture hybride** : Site mutualis√© + APIs externes
‚úÖ **Multi-API** : Groq (principal) + Gemini (backup) pour cumul quotas
‚úÖ **Historique** : Stockage BDD obligatoire pour contexte conversationnel
‚úÖ **Optimisation tokens** : Compression intelligente √©conomise 50-60%
‚úÖ **Capacit√©** : 16 000 conversations/jour gratuitement
‚úÖ **Scalable** : √âvolution facile selon croissance

Le projet est techniquement r√©alisable, √©conomiquement viable (gratuit au d√©marrage), et offre une excellente base pour un chatbot professionnel.

---

**Document cr√©√© pour Bruno - Janvier 2025**
**√Ä utiliser avec Claude dans VSCode pour impl√©mentation pratique**
